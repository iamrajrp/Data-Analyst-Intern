{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313eda68-53ca-4116-99df-6c697fc74790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite files found: ['2019-01.sqlite', '2019-02.sqlite', '2019-03.sqlite', '2019-04.sqlite', '2019-05.sqlite', '2019-06.sqlite', '2019-07.sqlite', '2019-08.sqlite', '2019-09.sqlite', '2019-10.sqlite', '2019-11.sqlite', '2019-12.sqlite']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = r\"C:\\After Backup\\Virtual Internship\\Week 6\\2019\"\n",
    "\n",
    "sqlite_files = [file for file in os.listdir(folder_path) if file.endswith(\".sqlite\")]\n",
    "print(\"SQLite files found:\", sqlite_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d6930e-f2c3-4a76-b9b4-35cb43bf94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in 2019-01.sqlite:\n",
      "       name\n",
      "0  tripdata\n"
     ]
    }
   ],
   "source": [
    "sample_db_path = os.path.join(folder_path, '2019-01.sqlite')\n",
    "\n",
    "conn = sqlite3.connect(sample_db_path)\n",
    "\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query, conn)\n",
    "\n",
    "print(\"Tables in 2019-01.sqlite:\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942dc8a2-54d2-4623-aa27-c83cea36e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 2019-01.sqlite to 2019-01.csv\n",
      "Converting 2019-02.sqlite to 2019-02.csv\n",
      "Converting 2019-03.sqlite to 2019-03.csv\n",
      "Converting 2019-04.sqlite to 2019-04.csv\n",
      "Converting 2019-05.sqlite to 2019-05.csv\n",
      "Converting 2019-06.sqlite to 2019-06.csv\n",
      "Converting 2019-07.sqlite to 2019-07.csv\n",
      "Converting 2019-08.sqlite to 2019-08.csv\n",
      "Converting 2019-09.sqlite to 2019-09.csv\n",
      "Converting 2019-10.sqlite to 2019-10.csv\n",
      "Converting 2019-11.sqlite to 2019-11.csv\n",
      "Converting 2019-12.sqlite to 2019-12.csv\n",
      "All SQLite files converted to CSVs.\n"
     ]
    }
   ],
   "source": [
    "for file in sqlite_files:\n",
    "    db_path = os.path.join(folder_path, file)\n",
    "    csv_name = file.replace(\".sqlite\", \".csv\")\n",
    "    csv_path = os.path.join(folder_path, csv_name)\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Already exists: {csv_name} â€” skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Converting {file} to {csv_name}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM tripdata\", conn)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    conn.close()\n",
    "\n",
    "print(\"All SQLite files converted to CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17145b8-f559-4be2-9712-922b1e166d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly CSVs found: ['2019-01.csv', '2019-02.csv', '2019-03.csv', '2019-04.csv', '2019-05.csv', '2019-06.csv', '2019-07.csv', '2019-08.csv', '2019-09.csv', '2019-10.csv', '2019-11.csv', '2019-12.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "csv_files.sort()\n",
    "print(\"Monthly CSVs found:\", csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219324aa-4ec5-4a41-83f9-ab5ec93b1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r\"C:\\After Backup\\Virtual Internship\\Week 6\\2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8c79b1-5a2d-4eeb-abe6-f8a4486120be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files: ['2019-01.csv', '2019-02.csv', '2019-03.csv', '2019-04.csv', '2019-05.csv', '2019-06.csv', '2019-07.csv', '2019-08.csv', '2019-09.csv', '2019-10.csv', '2019-11.csv', '2019-12.csv', 'combined_2019.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "csv_files.sort()\n",
    "print(\"CSV files:\", csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8383aebc-199a-4028-aa34-39d994c964d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files to combine: ['2019-01.csv', '2019-02.csv', '2019-03.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = sorted([file for file in os.listdir(folder_path) if file.endswith(\".csv\")])\n",
    "csv_files = csv_files[:3]  # Only keep the first 3 files to keep it >2GB but avoid system overloading\n",
    "print(\"CSV files to combine:\", csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c9e2f1-9971-415e-ac9c-5d8c2aa618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2019-01.csv\n",
      "Processing 2019-02.csv\n",
      "Processing 2019-03.csv\n"
     ]
    }
   ],
   "source": [
    "first_file = True\n",
    "combined_csv_path = os.path.join(folder_path, \"combined_3_months.csv\")\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    print(f\"Processing {file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False, on_bad_lines='skip')\n",
    "        df.to_csv(combined_csv_path, mode='w' if first_file else 'a',\n",
    "                  index=False, header=first_file)\n",
    "        first_file = False\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1f318e-3b83-425f-962e-ae5a2a0841c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2.58 GB\n"
     ]
    }
   ],
   "source": [
    "combined_file_path = os.path.join(folder_path, \"combined_3_months.csv\")\n",
    "size_in_gb = os.path.getsize(combined_file_path) / (1024**3)\n",
    "\n",
    "print(f\"File size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac9a59f-8dc5-47e5-b4fe-ba93023a7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a855f4de-1693-475d-9fab-d6fe56cfc9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: Loaded 22519712 rows and 18 columns\n",
      "Time taken: 176.01 seconds\n"
     ]
    }
   ],
   "source": [
    "@timer\n",
    "def read_with_pandas():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(combined_file_path)\n",
    "    print(\"Pandas: Loaded\", df.shape[0], \"rows and\", df.shape[1], \"columns\")\n",
    "    return df\n",
    "\n",
    "df_pandas = read_with_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0c80a6-e5e3-4c61-97b0-ac79d79b438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask: DataFrame created (delayed loading)\n",
      "Shape (approx): (<dask_expr.expr.Scalar: expr=ArrowStringConversion(frame=FromMapProjectable(e891dea)).size() // 18, dtype=int64>, 18)\n",
      "Time taken: 9.55 seconds\n"
     ]
    }
   ],
   "source": [
    "@timer\n",
    "def read_with_dask():\n",
    "    import dask.dataframe as dd\n",
    "    df = dd.read_csv(combined_file_path)\n",
    "    print(\"Dask: DataFrame created (delayed loading)\")\n",
    "    print(\"Shape (approx):\", df.shape)\n",
    "    return df\n",
    "\n",
    "df_dask = read_with_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c0db6-d09d-4f60-944d-be7bfef40553",
   "metadata": {},
   "source": [
    "**Modin could not be benchmarked due to environment-specific issues with Ray on Jupyter under Windows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d839118a-9bc7-4ea5-829c-0ec8a0eaf6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample loaded: (1000, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = r\"C:\\After Backup\\Virtual Internship\\Week 6\\2019\"\n",
    "combined_file_path = os.path.join(folder_path, \"combined_3_months.csv\")\n",
    "\n",
    "df_pandas = pd.read_csv(combined_file_path, nrows=1000, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "print(\"Sample loaded:\", df_pandas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9ef000-7c17-412b-83c7-71a01cfe0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
      "After cleaning: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning:\", df_pandas.columns.tolist())\n",
    "\n",
    "df_pandas.columns = df_pandas.columns.str.strip().str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "print(\"After cleaning:\", df_pandas.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2aef98-f3b1-4aa6-abff-773b8798bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML schema saved at: C:\\After Backup\\Virtual Internship\\Week 6\\2019\\schema.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "schema = {\n",
    "    \"separator\": \",\",\n",
    "    \"columns\": df_pandas.columns.tolist()\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(folder_path, \"schema.yaml\")\n",
    "\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(schema, f)\n",
    "\n",
    "print(\"YAML schema saved at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57115f5-c625-4132-8bc4-e6388c6f59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Columns from YAML: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
      "Separator: ,\n"
     ]
    }
   ],
   "source": [
    "with open(yaml_path, \"r\") as f:\n",
    "    loaded_schema = yaml.safe_load(f)\n",
    "\n",
    "expected_columns = loaded_schema[\"columns\"]\n",
    "separator = loaded_schema[\"separator\"]\n",
    "\n",
    "print(\"Expected Columns from YAML:\", expected_columns)\n",
    "print(\"Separator:\", separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2862d1aa-27cd-463f-aea4-dd68534213dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name and count match the YAML schema\n"
     ]
    }
   ],
   "source": [
    "# Read just 5 rows to get column names\n",
    "df_check = pd.read_csv(combined_file_path, sep=separator, nrows=5, on_bad_lines='skip')\n",
    "\n",
    "actual_columns = df_check.columns.tolist()\n",
    "\n",
    "# Check column match\n",
    "if actual_columns == expected_columns:\n",
    "    print(\"Column name and count match the YAML schema\")\n",
    "else:\n",
    "    print(\"Mismatch!\")\n",
    "    print(\"From CSV:\", actual_columns)\n",
    "    print(\"From YAML:\", expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3958ac5b-8ea8-48fc-a72c-d2d7016adc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully written to: C:\\After Backup\\Virtual Internship\\Week 6\\2019\\combined_3_months_cleaned.txt.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "gz_output_path = os.path.join(folder_path, \"combined_3_months_cleaned.txt.gz\")\n",
    "\n",
    "columns = expected_columns\n",
    "\n",
    "first_chunk = True\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "reader = pd.read_csv(combined_file_path, chunksize=chunk_size, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "with gzip.open(gz_output_path, 'wt', encoding='utf-8') as f_out:\n",
    "    for chunk in reader:\n",
    "        chunk.columns = chunk.columns.str.strip().str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "        chunk.to_csv(f_out, sep='|', index=False, header=first_chunk)\n",
    "        first_chunk = False\n",
    "\n",
    "print(\"File successfully written to:\", gz_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987527f4-1d69-4f03-b0d1-a0ef055f824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample .gz file created: C:\\After Backup\\Virtual Internship\\Week 6\\2019\\sample_100k_cleaned.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "sample_df = pd.read_csv(combined_file_path, nrows=100000, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "sample_df.columns = sample_df.columns.str.strip().str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "sample_output_path = os.path.join(folder_path, \"sample_100k_cleaned.gz\")\n",
    "\n",
    "sample_df.to_csv(sample_output_path, sep=\"|\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Sample .gz file created:\", sample_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a7d0a5-f7a8-4c26-baa9-7ab09978c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL SUMMARY:\n",
      "Total Rows           : 22519712\n",
      "Total Columns        : 18\n",
      "File Size            : 389.38 MB\n",
      "File Path            : C:\\After Backup\\Virtual Internship\\Week 6\\2019\\combined_3_months_cleaned.txt.gz\n"
     ]
    }
   ],
   "source": [
    "row_count = 0\n",
    "chunk_size = 100000\n",
    "\n",
    "for chunk in pd.read_csv(gz_output_path, sep='|', chunksize=chunk_size, compression='gzip', on_bad_lines='skip'):\n",
    "    row_count += len(chunk)\n",
    "\n",
    "df_sample = pd.read_csv(gz_output_path, sep='|', nrows=5, compression='gzip', on_bad_lines='skip')\n",
    "column_count = df_sample.shape[1]\n",
    "\n",
    "file_size_mb = os.path.getsize(gz_output_path) / (1024 ** 2)\n",
    "\n",
    "print(\"\\nFINAL SUMMARY:\")\n",
    "print(\"Total Rows           :\", row_count)\n",
    "print(\"Total Columns        :\", column_count)\n",
    "print(f\"File Size            : {file_size_mb:.2f} MB\")\n",
    "print(\"File Path            :\", gz_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341fc19-27f8-4fca-b005-635ff489f62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
